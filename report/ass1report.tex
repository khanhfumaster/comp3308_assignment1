\documentclass{article}
\usepackage{fancyhdr} 
\usepackage{lastpage}
\usepackage{mathtools}
\usepackage{extramarks}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{courier}
\usepackage{lipsum} 
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{url}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{COMP3308}
\chead{Introduction to Artificial Intelligence}
\rhead{Assignment 1}
\lfoot{}
\cfoot{\thepage}
\rfoot{Woo Hyun Jung 310250811 \\  Khanh Cao Quoc Nguyen 311253865} 
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\renewcommand{\tt}{\texttt}
\setlength\parindent{0pt} 

\title{COMP3308 Assignment 1 \\ Predicting Diabetes}
\author{Woo Hyun Jung 310250811 \\  Khanh Cao Quoc Nguyen 311253865}
\date{}
\begin{document}
\maketitle
\thispagestyle{empty}
\newpage

\section{Aim}
The aim of our study is to predict whether a new patient will test positive for diabetes (class 1). The study is important because blah blah dicks

\section{Data}
\subsection{Dataset}
The data set we used was the Pima Indians Diabetes Database found at \url{http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/}. \\
There are nine attributes: 
\begin{enumerate}[1.]
\item Number of times pregnant
\item Plasma glucose concentration a 2 hours in an oral glucose tolerance test
\item Diastolic blood pressure (mm Hg)
\item Triceps skin fold thickness (mm)
\item 2-Hour serum insulin (mu U/ml)
\item Body mass index (weight in kg/(height in m)$^2)$
\item Diabetes pedigree function
\item Age (years)
\item Class variable (0 or 1)
\end{enumerate} 
and two classes:
\begin{enumerate}[1.]
\item class1 (testing positive for diabetes)
\item class0 (testing negative for diabetes)
\end{enumerate} 

\subsection{Data preparation}
\subsubsection{Manual preprocessing}
The raw \tt{.data} file was preprocessed manually in multiple ways. Firstly we had to come up with simple names for the nine attributes:
\begin{enumerate}[1.]
\item num\_pregnant
\item plasma\_glucose\_concentration
\item diastolic\_blood\_pressure
\item tricep\_skin\_fold\_thickness
\item 2h\_serum\_insulin
\item bmi
\item diabetes\_pedigree\_function
\item age
\item class
\end{enumerate}
Using these names, we added a header row the \tt{.data} file. We then changed the values of the class column to class0 and class1 instead of 0 and 1. This file was then saved a \tt{.csv} file for later convenience.

\subsubsection{Missing Values}
We wrote a script \tt{csv\_scripts\textbackslash missing\_values.py} that dealt with the missing values in the following attributes:
\begin{enumerate}[1.]
\item plasma\_glucose\_concentration
\item diastolic\_blood\_pressure
\item tricep\_skin\_fold\_thickness
\item 2h\_serum\_insulin
\item bmi
\item diabetes\_pedigree\_function
\end{enumerate} 

We determined that these attributes consisted of missing values because 0 is a impossible or unrealistic value for it.\\

This script outputs two \tt{.csv} files, one with missing values taking the calculated average of its respective attribute(\tt{pima-indians-diabetes-avg.csv}) and the other with any instances with missing values omitted(\tt{pima-indians-diabetes-omit.csv}).\\

We noticed that if we omit the instances with missing values we would be left with 392 instances out of the original 768. So we decided to use the averaged output file for the rest of the assignment.

\subsubsection{Normalisation}
Once this new file was loaded into Weka, each of the attributes were normalised in the range $[0, 1]$. The final output file was saved as \tt{pima.csv}

\subsection{Attribute selection}
With the \tt{pima.csv} loaded into Weka, we used the \tt{CfsSubsetEval} attribute evaluator with the \tt{BestFirst} search method available on the \tt{Select attributes} tab to determine the best subset of features based on how good the individual feature are at predicting the class and how much they correlate with the other features.
The selected attributes were:
\begin{enumerate}[1.]
\item plasma\_glucose\_concentration
\item 2h\_serum\_insulin
\item bmi
\item diabetes\_pedigree\_function
\item age
\end{enumerate} 

We then proceed to use Weka's preprocessing features to remove the attributes that were not selected and saved the file as \tt{pima-CFS.csv}.

\section{Results and Discussion}
\begin{table}[h]
\begin{tabular}{llllllll}
\hline
                                    & ZeroR   & 1R      & 1-NN    & 5-NN    & NB      & DT      & MLP     \\ \hline
No feature selection                & 65.1042 & 70.8333 & 67.7083 & 74.7396 & 74.7396 & 72.2656 & 75.3906 \\
Correlation-based feature selection & 65.1042 & 70.8333 & 69.0104 & 74.6094 & 76.4323 & 73.9583 & 75.7813 \\ \hline
\end{tabular}
\caption{Results from Weka}
\end{table}

\begin{table}[h]
\begin{tabular}{@{}llll@{}}
\hline
                                    & My1-NN  & My5-NN  & MyNB    \\ \hline
No feature selection                & 68.7645 & 75.2546 & 75.3828 \\
Correlation-based feature selection & 68.4979 & 76.0509 & 76.6883 \\ \hline
\end{tabular}
\caption {Results of implementations of kNN and NB}
\end{table}

discuss here blah blah blah bleh

\section{Conclusions}
\section{Reflection}
\section{Instructions}
Our implementation of K Nearest Neighbour, Naive Bayes and 10\-fold stratified cross validation was written in Python 2.7.5.\\

The \tt{ai\_main.py} file runs 10-fold stratified cross validation on K Nearest Neighbour($k=1$ and $k=5$) and Naive Bayes on a given \tt{.csv} file.\\

To run our implementation, you must \tt{cd} into the \tt{ai\_code} directory and ensure that \tt{pima.csv} and \tt{pima-CFS.csv} are present in the directory. Then run:
\begin{lstlisting}
python ai_main.py pima.csv
python ai_main.py pima-CFS.csv
\end{lstlisting}

This should output \tt{pima-folds.csv} and \tt{pima-CFS-folds.csv} respectively, which are the stratified folds.












\end{document}